{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Embedding, LSTM, Dense\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'wikiner2013inflected', 'source': '1-1', 'alignment_type': '1.000', 'alignment_quality': 'Sharaabi', 'translation': {'en': 'Sharaabi', 'hi': 'शराबी'}}\n",
      "Hindi: शराबी\n",
      "273885\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "dataset_list = []\n",
    "\n",
    "with open(\"hindencorp05.plaintext\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        splits = line.strip().split(\"\\t\")\n",
    "        if len(splits) == 5:\n",
    "            dataset_list.append({\n",
    "                \"id\": splits[0],\n",
    "                \"source\": splits[1],\n",
    "                \"alignment_type\": splits[2],\n",
    "                \"alignment_quality\": splits[3],\n",
    "                \"translation\": {\"en\": splits[3], \"hi\": splits[4]},\n",
    "            })\n",
    "\n",
    "print(dataset_list[0])  # Print the first 5 example\n",
    "print(\"Hindi:\", dataset_list[0]['translation']['hi'])\n",
    "print(len(dataset_list))\n",
    "dataset_list = dataset_list[:100]\n",
    "print(len(dataset_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 English Sentences: ['sharaabi', 'politicians do not have permission to do what needs to be done.', \"i'd like to tell you about one such child,\", 'this percentage is even greater than the percentage in india.', '- john collins']\n",
      "100 Hindi Sentences: ['शराबी', 'राजनीतिज्ञों के पास जो कार्य करना चाहिए, वह करने कि अनुमति नहीं है .', 'मई आपको ऐसे ही एक बच्चे के बारे में बताना चाहूंगी,', 'यह प्रतिशत भारत में हिन्दुओं प्रतिशत से अधिक है।', '- जॉन कॉलिन्स']\n"
     ]
    }
   ],
   "source": [
    "def preprocess_dataset(dataset):\n",
    "    eng_sentences = []\n",
    "    hin_sentences = []\n",
    "    for entry in dataset:\n",
    "        eng_sentences.append(entry['translation']['en'].lower())\n",
    "        hin_sentences.append(entry['translation']['hi'])\n",
    "    return eng_sentences, hin_sentences\n",
    "\n",
    "# Preprocess the dataset\n",
    "eng_sentences, hin_sentences = preprocess_dataset(dataset_list)\n",
    "\n",
    "print(len(eng_sentences),\"English Sentences:\", eng_sentences[:5])\n",
    "print(len(hin_sentences),\"Hindi Sentences:\", hin_sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_sentences = [\"<start> \" + s + \" <end>\" for s in eng_sentences]\n",
    "hin_sentences = [\"<start> \" + s + \" <end>\" for s in hin_sentences]\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "def tokenize(sentences):\n",
    "    tokenizer = Tokenizer(filters=\"\")\n",
    "    tokenizer.fit_on_texts(sentences)\n",
    "    sequences = tokenizer.texts_to_sequences(sentences)\n",
    "    return sequences, tokenizer\n",
    "\n",
    "eng_sequences, eng_tokenizer = tokenize(eng_sentences)\n",
    "hin_sequences, hin_tokenizer = tokenize(hin_sentences)\n",
    "\n",
    "eng_sequences = pad_sequences(eng_sequences, padding='post')\n",
    "hin_sequences = pad_sequences(hin_sequences, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "hin_vocab_size = len(hin_tokenizer.word_index) + 1\n",
    "\n",
    "# Model Parameters\n",
    "embedding_dim = 256\n",
    "units = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder\n",
    "class Encoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        output, state_h, state_c = self.lstm(x)\n",
    "        return output, state_h, state_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention\n",
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.W1 = Dense(units)\n",
    "        self.W2 = Dense(units)\n",
    "        self.V = Dense(1)\n",
    "\n",
    "    def call(self, query, values):\n",
    "        query_with_time_axis = tf.expand_dims(query, 1)\n",
    "        score = self.V(tf.nn.tanh(self.W1(query_with_time_axis) + self.W2(values)))\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder\n",
    "class Decoder(Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = LSTM(units, return_sequences=True, return_state=True)\n",
    "        self.fc = Dense(vocab_size)\n",
    "        self.attention = BahdanauAttention(units)\n",
    "\n",
    "    def call(self, x, enc_output, state_h, state_c):\n",
    "        context_vector, attention_weights = self.attention(state_h, enc_output)\n",
    "        x = self.embedding(x)\n",
    "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state=[state_h, state_c])\n",
    "        output = self.fc(output)\n",
    "        return output, state_h, state_c, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate models\n",
    "encoder = Encoder(eng_vocab_size, embedding_dim, units)\n",
    "decoder = Decoder(hin_vocab_size, embedding_dim, units)\n",
    "\n",
    "# Loss function and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.not_equal(real, 0)\n",
    "    loss = loss_object(real, pred)\n",
    "    return tf.reduce_mean(loss * tf.cast(mask, dtype=loss.dtype))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inp, targ, enc_hidden):\n",
    "    loss = 0\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Encode input\n",
    "        enc_output, enc_hidden_h, enc_hidden_c = encoder(inp)\n",
    "        dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "        \n",
    "        # Initial decoder input (<start> tokens)\n",
    "        dec_input = tf.expand_dims([hin_tokenizer.word_index['<start>']] * targ.shape[0], 1)\n",
    "        \n",
    "        # Iterate through each timestep\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # Forward pass\n",
    "            predictions, dec_hidden_h, dec_hidden_c, _ = decoder(dec_input, enc_output, dec_hidden_h, dec_hidden_c)\n",
    "            \n",
    "            # Adjust shape for loss calculation (remove time dimension)\n",
    "            predictions = tf.squeeze(predictions, axis=1)  # Shape: (batch_size, vocab_size)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            \n",
    "            # Use the true target as the next decoder input\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "    \n",
    "    # Normalize the loss\n",
    "    batch_loss = loss / int(targ.shape[1])\n",
    "    \n",
    "    # Backpropagation\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss 1.1214\n",
      "Epoch 2 Loss 0.9258\n",
      "Epoch 3 Loss 1.0127\n",
      "Epoch 4 Loss 0.9488\n",
      "Epoch 5 Loss 0.8837\n",
      "Epoch 6 Loss 0.9647\n",
      "Epoch 7 Loss 0.9644\n",
      "Epoch 8 Loss 0.9235\n",
      "Epoch 9 Loss 1.0321\n",
      "Epoch 10 Loss 1.0486\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((eng_sequences, hin_sequences)).shuffle(len(eng_sequences))\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    for (batch, (inp, targ)) in enumerate(dataset):\n",
    "        batch_loss = train_step(inp, targ, None)\n",
    "        total_loss += batch_loss\n",
    "    print(f'Epoch {epoch+1} Loss {total_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "save_path = \"translator_model/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "# Save encoder and decoder weights\n",
    "encoder.save_weights(os.path.join(save_path, \"encoder.weights.h5\"))\n",
    "decoder.save_weights(os.path.join(save_path, \"decoder.weights.h5\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess_sentence(sentence):\n",
    "    sentence = sentence.lower().strip()\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r\"[^a-zA-Z?.!,]+\", r\" \", sentence)\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()\n",
    "    return sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder, eng_tokenizer, hin_tokenizer, max_length_targ):\n",
    "    # Preprocess the input sentence\n",
    "    sentence = preprocess_sentence(sentence)  # Ensure it's lowercased and cleaned\n",
    "    inputs = eng_tokenizer.texts_to_sequences([sentence])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, maxlen=max_length_targ, padding='post')\n",
    "    inputs = tf.convert_to_tensor(inputs)\n",
    "    \n",
    "    # Encode the input sentence\n",
    "    enc_out, enc_hidden_h, enc_hidden_c = encoder(inputs)\n",
    "    \n",
    "    # Prepare for decoding\n",
    "    dec_hidden_h, dec_hidden_c = enc_hidden_h, enc_hidden_c\n",
    "    dec_input = tf.expand_dims([hin_tokenizer.word_index['<start>']], 0)\n",
    "    \n",
    "    # Generate translation\n",
    "    result = []\n",
    "    for _ in range(max_length_targ):\n",
    "        predictions, dec_hidden_h, dec_hidden_c, _ = decoder(dec_input, enc_out, dec_hidden_h, dec_hidden_c)\n",
    "        predictions = tf.argmax(predictions, axis=-1).numpy()\n",
    "        \n",
    "        # Get the predicted word index\n",
    "        predicted_id = predictions[0][0]\n",
    "        if hin_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            break\n",
    "        \n",
    "        # Append the word to the result\n",
    "        result.append(hin_tokenizer.index_word[predicted_id])\n",
    "        \n",
    "        # Use the predicted word as the next decoder input\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "    \n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: How are you?\n",
      "Translation: \n"
     ]
    }
   ],
   "source": [
    "# Example input\n",
    "input_sentence = \"How are you?\"\n",
    "\n",
    "# Translate\n",
    "translated_sentence = translate(input_sentence, encoder, decoder, eng_tokenizer, hin_tokenizer, max_length_targ=20)\n",
    "print(f'Input: {input_sentence}')\n",
    "print(f'Translation: {translated_sentence}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
